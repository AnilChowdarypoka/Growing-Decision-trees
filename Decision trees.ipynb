{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39cac060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import mode\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC , LinearSVC\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46d79068",
   "metadata": {},
   "source": [
    "Train and fine-tune a Decision Tree for the moons dataset by following these steps:\n",
    "\n",
    "    a. Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset.\n",
    "    b. Split it into a training set and a test set using train_test_split().\n",
    "    c. Use grid search with cross-validation (with the help of the GridSearchCV class) to find\n",
    "    good hyperparameter values for a DecisionTreeClassifier.\n",
    "    Hint: try various values for max_leaf_nodes.\n",
    "    d. Train it on the full training set using these hyperparameters, and measure your model’s\n",
    "    performance on the test set. You should get roughly 85% to 87% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ce4bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    point_1   point_2  target\n",
      "0  0.940291  0.122306       1\n",
      "1  0.124540 -0.424775       0\n",
      "2  0.261988  0.508414       0\n",
      "3 -0.495238  0.072589       0\n",
      "4 -0.879413  0.549373       0\n"
     ]
    }
   ],
   "source": [
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "data = pd.DataFrame(data=X, columns=['point_1', 'point_2'])\n",
    "data['target'] = y\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0556d9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test data shapes (7000, 2) , (3000, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_data(feature , dependent, test_size = 0.3,  random_state = 20):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                                    feature,  dependent,  test_size = test_size,  random_state = random_state)\n",
    "    print(f\"Train and test data shapes {X_train.shape} , {X_test.shape}\" , end = \"\\n\\n\")\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(data.drop(['target'] , axis = 1) , data[['target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284d64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tune_model(classifier , param_grid):\n",
    "    grid_search = GridSearchCV(classifier,param_grid=param_grid)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_ , grid_search.best_params_\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],      \n",
    "    'splitter': ['best', 'random'],         \n",
    "    'max_depth': [None, 10, 20, 30],           \n",
    "    'min_samples_split': [2, 5, 10],           \n",
    "    'min_samples_leaf': [1, 2, 4]             \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79b051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_tuned  , decision_tree_best_param= tune_model(DecisionTreeClassifier(random_state=42) , param_grid )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9754753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree best Params  : {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'splitter': 'random'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Decision tree best Params  : {decision_tree_best_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42fe7b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of Decision Tree with parameters {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'splitter': 'random'} is 86.7\n"
     ]
    }
   ],
   "source": [
    "def evaluate_classification(model, X_train = X_train, X_test = X_test, Y_train =y_train , Y_test = y_test , is_fitted =True):\n",
    "    if is_fitted:\n",
    "        model.fit(X_train,Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, predictions)\n",
    "    return accuracy , predictions\n",
    "\n",
    "accuracy , _ = evaluate_classification(decision_tree_tuned , X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f\"The Accuracy of Decision Tree with parameters {decision_tree_best_param} is {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0de88bb",
   "metadata": {},
   "source": [
    "Grow a forest.\n",
    "\n",
    "    a. Continuing the previous exercise, generate 1,000 subsets of the training set, each\n",
    "    containing 100 instances selected randomly. Hint: you can use Scikit-Learn’s ShuffleSplit\n",
    "    class for this.\n",
    "    \n",
    "    b. Train one Decision Tree on each subset, using the best hyperparameter values found\n",
    "    above. Evaluate these 1,000 Decision Trees on the test set. Since they were trained on\n",
    "    smaller sets, these Decision Trees will likely perform worse than the first Decision Tree,\n",
    "    achieving only about 80% accuracy.\n",
    "    \n",
    "    c. Now comes the magic. For each test set instance, generate the predictions of the 1,000\n",
    "    Decision Trees, and keep only the most frequent prediction (you can use SciPy’s mode()\n",
    "    function for this). This gives you majority-vote predictions over the test set.\n",
    "    \n",
    "    d. Evaluate these predictions on the test set: you should obtain a slightly higher accuracy\n",
    "    than your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a\n",
    "    Random Forest classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9042c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_trees = 1000\n",
    "num_of_instances = 100\n",
    "\n",
    "def shuffle_data(trees , instances):\n",
    "    \n",
    "    subsets = []\n",
    "    shuffle_split_object = ShuffleSplit(n_splits=trees, random_state=42)\n",
    "\n",
    "    for index, _ in shuffle_split_object.split(X_train):\n",
    "        subsets.append((X_train.iloc[index], y_train.iloc[index]))\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "subsets = shuffle_data(num_of_trees , num_of_instances )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1893e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_tuned = DecisionTreeClassifier(**decision_tree_best_param)\n",
    "trees_forest = [clone(decision_tree_tuned) for _ in range(num_of_trees)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41bdd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_accuracy(forest , subset):\n",
    "    scores = []\n",
    "    for tree, (x,y) in zip(forest , subset):\n",
    "        scores.append(evaluate_classification(tree , X_train  = x , Y_train = y))\n",
    "    return scores\n",
    "\n",
    "accuracy_scores =  compute_accuracy(trees_forest , subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95269603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of Decision Tree with majority vote is 86.96666666666667\n"
     ]
    }
   ],
   "source": [
    "def compute_majority_votes():\n",
    "    \n",
    "    sample_prediction_matrix = np.zeros((num_of_trees, len(X_test)))\n",
    "    for tree_count, model_tree in enumerate(trees_forest):\n",
    "        sample_prediction_matrix[tree_count] = model_tree.predict(X_test)\n",
    "\n",
    "    predicted_majority_votes, _ = mode(sample_prediction_matrix, axis=0)\n",
    "\n",
    "    majority_votes_accuracy = accuracy_score(y_test, predicted_majority_votes.reshape([-1]))\n",
    "    \n",
    "    return majority_votes_accuracy\n",
    "\n",
    "print(f\"The Accuracy of Decision Tree with majority vote is {compute_majority_votes()*100}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a64621bc",
   "metadata": {},
   "source": [
    "Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set,\n",
    "and a test set (e.g., use the first 40,000 instances for training, the next 10,000 for validation, and\n",
    "the last 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, an\n",
    "Extra-Trees classifier, and an SVM. Next, try to combine them into an ensemble that outperforms\n",
    "them all on the validation set, using a soft or hard voting classifier. Once you have found one, try\n",
    "it on the test set. How much better does it perform compared to the individual classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "128637ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"].values.astype(int), mnist[\"target\"].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aa9a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test data shapes (60000, 784) , (10000, 784)\n",
      "\n",
      "Train and test data shapes (50000, 784) , (10000, 784)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X  ,y , test_size=10000)\n",
    "\n",
    "X_train, X_val, y_train, y_val = split_data(X_train  ,y_train , test_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "873543cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initilize_classification_models():\n",
    "    extra_tree_classifier = ExtraTreesClassifier( n_estimators=100,random_state=42)\n",
    "    random_forest_classifier = RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "    svc_classifier = LinearSVC(dual=False)\n",
    "\n",
    "\n",
    "    return extra_tree_classifier , random_forest_classifier , svc_classifier\n",
    "\n",
    "extra_tree_classifier , random_forest_classifier , svc_classifier = initilize_classification_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18b7aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting done\n",
      "The Accuracy of extra_tree_classifier is 97.21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "extra_tree_accuracy , extra_tree_predictions = evaluate_classification(extra_tree_classifier , X_train, X_val, y_train, y_val)\n",
    "print(f\"The Accuracy of extra_tree_classifier is {extra_tree_accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "941837fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting done\n",
      "The Accuracy of random_forest_classifier is 96.74000000000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random_forest_accuracy , random_forest_predictions = evaluate_classification(random_forest_classifier , X_train, X_val, y_train, y_val)\n",
    "print(f\"The Accuracy of random_forest_classifier is {random_forest_accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173390f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(probability=True, random_state=42)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_pred = svm_clf.predict(X_test)\n",
    "svm_acc = accuracy_score(y_test, svm_pred)\n",
    "svm_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ee295",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_accuracy , svc_predictions = evaluate_classification(svc_classifier , X_train, X_val, y_train, y_val)\n",
    "print(f\"The Accuracy of svc_classifier is {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be7415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[('random_forest_classifier', random_forest_classifier),\n",
    "                ('extra_tree_classifier', extra_tree_classifier), ('svm', svc_classifier)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "voting_classifier_accuracy_val , voting_classifier_val_predictions = evaluate_classification(voting_classifier ,\n",
    "                                                                             X_train, X_val, y_train, y_val , True)\n",
    "\n",
    "print(\"Ensemble Accuracy on Validation Set:\", voting_classifier_accuracy_val)\n",
    "\n",
    "\n",
    "voting_classifier_accuracy_test , voting_classifier_test_predictions = evaluate_classification(voting_classifier ,\n",
    "                                                         X_train, X_test, y_train, y_test , True)\n",
    "print()\n",
    "print(\"Ensemble Accuracy on Test Set:\", voting_classifier_accuracy_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a93638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0f292aca",
   "metadata": {},
   "source": [
    "Run the individual classifiers from the previous exercise to make predictions on the validation\n",
    "set and create a new training set with the resulting predictions: each training instance is a vector\n",
    "containing the set of predictions from all your classifiers for an image, and the target is the\n",
    "image’s class. Congratulations, you have just trained a blender, and together with the classifiers\n",
    "they form a stacking ensemble! Now let’s evaluate the ensemble on the test set. For each image\n",
    "in the test set, make predictions with all your classifiers, then feed the predictions to the blender\n",
    "to get the ensemble’s predictions. How does it compare to the voting classifier you trained\n",
    "earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3434bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_forest_classifier_test_prediction = random_forest_classifier.predict(X_test)\n",
    "extra_tree_classifier_test_prediction = extra_tree_classifier.predict(X_test)\n",
    "svm_test_prediction = svm_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blender_train_data = np.column_stack((extra_tree_predictions, random_forest_predictions , svm_val_pred))\n",
    "blender_test_data = np.column_stack((rf_test_pred, et_test_pred , svm_test_pred))\n",
    "\n",
    "\n",
    "blender = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "blender.fit(blender_train_data, y_val)\n",
    "blender_predictions = blender.predict(blender_test_data)\n",
    "blender_accuracy = accuracy_score(y_test, blender_predictions)\n",
    "\n",
    "print(\"Ensemble Accuracy with Stacking on Test Set:\", blender_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c067fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm_clf = LinearSVC(dual=False, random_state=42, max_iter=1000, C=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm_clf.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef1457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
